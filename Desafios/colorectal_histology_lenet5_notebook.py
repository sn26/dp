# -*- coding: utf-8 -*-
"""Colorectal_histology_LeNet5_notebook.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/15ntjdxS5eK5mCg3pr1GwpbhMan-js0J4

# Colorectal histology dataset with LeNet 5

# Preview: Neural Network examples with `tf.keras`

- `tf.keras` Software (http://keras.io/);

- Visual example of LeNet-5 (http://yann.lecun.com/exdb/lenet/) using MNIST digits dataset;

- Examples with common neural network topologies (https://machinelearningmastery.com/tensorflow-tutorial-deep-learning-with-tf-keras/).

## Colorectal histology

Methods to diagnose colorectal using histology images (<https://zenodo.org/record/53169#.XGZemKwzbmG>, <https://www.tensorflow.org/datasets/catalog/colorectal_histology>)

In this case, the purpose is to classify the type of histology in a given image in the following categories:

- 0: TUMOR
- 1: STROMA
- 2: COMPLEX
- 3: LYMPHO
- 4: DEBRIS
- 5: MUCOSA
- 6: ADIPOSE
- 7: EMPTY

## Local instalation (option 1)

Install the following Python packages to run this notebook

`pip install pip -U`

`pip install tensorflow jupyter`

## Google Colab (option 2)

[Google Colab](https://colab.research.google.com/) is a research project created to help disseminate machine learning education and research. It's a `Jupyter notebook` environment that requires no setup to use and runs entirely in the cloud.

Colaboratory notebooks are stored in [Google Drive](https://drive.google.com) and can be shared just as you would with Google Docs or Sheets. Colaboratory is free to use.

For more information, see our [FAQ](https://research.google.com/colaboratory/faq.html).

### How install extra packages
Google Colab installs a series of basic packages if we need any additional package just install it.
"""



"""## Import packages"""
import scipy.linalg as LA
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import collections

import sklearn
from sklearn.model_selection import train_test_split
from sklearn import metrics

import tensorflow as tf
import tensorflow_datasets as tfds

from tensorflow import keras
from tensorflow.keras.preprocessing.image import array_to_img, img_to_array, load_img
from tensorflow.keras.models import Sequential
from tensorflow.keras import layers

"""## Define global constants

Lets start with a few epochs to test learning network parameters
"""
x_gemv = LA.get_blas_funcs('gemv', (0,))
batch_size = 32 #Number of training examples
nb_classes = 2  #or 8
epochs = 25 #Number of iterations // Passes the entire training has completed

# Scaling input image to theses dimensions
img_rows, img_cols = 32, 32

"""## Load image database"""

def format_example(image):
    image = tf.cast(image, tf.float32)
    # Normalize the pixel values
    image = image / 255.0
    # Resize the image
    image = tf.image.resize(image, (img_rows, img_cols))
    return image


def load_data(name="colorectal_histology"):
  train_ds = tfds.load(name, split=tfds.Split.TRAIN, batch_size=-1)
  train_ds['image'] = tf.map_fn(format_example, train_ds['image'], dtype=tf.float32)
  numpy_ds = tfds.as_numpy(train_ds)
  X, y = numpy_ds['image'], numpy_ds['label']

  return np.array(X), np.array(y)

"""## Plot images"""

def plot_symbols(X,y,n=15):
    index = np.random.randint(len(y), size=n)
    plt.figure(figsize=(25, 2))
    for i in np.arange(n):
        ax = plt.subplot(1,n,i+1)
        plt.imshow(X[index[i],:,:,:])
        plt.gray()
        ax.set_title(f'{y[index[i]]}-{index[i]}')
        ax.get_xaxis().set_visible(False)
        ax.get_yaxis().set_visible(False)
    plt.show()

"""## Build LeNet5 structure

<center><img src="https://www.dlsi.ua.es/~juanra/UA/curso_verano_DL/images/LeNet5.jpg"></center>
"""

#
# Build an ANN structure - LeNet5
#

def cnn_model():
    #
    # Neural Network Structure
    #
    
    model = Sequential()
    
    
    model.add(layers.Conv2D(6, (5, 5))) #Output dimensionality, kernel_size (Height, width)
    model.add(layers.Activation("sigmoid")) #Applies an activation function to an output
    model.add(layers.MaxPooling2D(pool_size=(2, 2))) #Getting a size 2x2 for the window
    
    model.add(layers.Conv2D(16, (5, 5)))
    model.add(layers.Activation("sigmoid"))
    model.add(layers.MaxPooling2D(pool_size=(2, 2)))
        
    model.add(layers.Flatten())
    
    model.add(layers.Dense(120)) #dense(Dimensionality of the output space)
    model.add(layers.Activation("sigmoid"))
    
    model.add(layers.Dense(84))
    model.add(layers.Activation("sigmoid"))
    
    model.add(layers.Dense(nb_classes))
    model.add(layers.Activation('softmax'))

    return model

"""## Start to run the program

### Load data
"""

##################################################################################
# Main program

X, y = load_data()

print(X.shape, 'train samples')
print(img_rows,'x', img_cols, 'image size')
print(epochs,'epochs')

"""Only for binary classification. All number of classes greater than 0 will be set to 1."""

if nb_classes==2:
  y[y>0] = 1

"""### Let to see examples of the dataset"""

plot_symbols(X, y, 15)

"""## Number of examples per class"""

collections.Counter(y)

"""## Split examples in training/test sets

This section is introductory to serve as a simple example. To test the model created in different situations, a 10 cross validation (10-CV) strategy should be used.
"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)

# Convert integers to one-hot vector
y_train_nn = keras.utils.to_categorical(y_train, nb_classes)
y_test_nn = keras.utils.to_categorical(y_test, nb_classes)

print(f'X_train {X_train.shape} X_test {X_test.shape}')
print(f'y_train {y_train.shape} y_test {y_test.shape}')
print(f'y_train_nn {y_train_nn.shape} y_test_nn {y_test_nn.shape}')

"""### Model and optimizers

Test [optimizer](https://keras.io/optimizers/) parameter with `sgd`, `adadelta` or `adam` values in order to check the final precision achieved.
"""

model = cnn_model()

model.compile(loss='categorical_crossentropy',optimizer='sgd', metrics=['accuracy'])

#early_stopping = EarlyStopping(monitor='val_loss', patience=10)
model.fit(X_train, y_train_nn, batch_size=batch_size, epochs=epochs, validation_split=0.1, verbose=2) #, callbacks=[early_stopping])

print(model.summary())

loss, acc = model.evaluate(X_test, y_test_nn, batch_size=batch_size)
print(f'loss: {loss:.2f} acc: {acc:.2f}')

"""## Prediction

### Testing AUC result for two and multiple classes
"""

y_scores = model.predict(X_test) # Confidence prediction per class
y_pred = y_scores.argmax(axis=1) # Select classes with most confidence prediction

if nb_classes ==  2:
  print(f'AUC {metrics.roc_auc_score(y_test, np.round(y_scores[:,1],2)):.4f} ')
else:
  print(f'AUC {metrics.roc_auc_score(y_test, y_scores, multi_class = "ovr"):.4f} ')

"""## More metrics about results

We can find more information about `precision`, `recall` and `f1` metrics in <https://en.wikipedia.org/wiki/Precision_and_recall>.
"""

print('Predictions', collections.Counter(y_pred),'\n')

print('Confusion matrix')
print(metrics.confusion_matrix(y_test,y_pred),'\n')

target_names = ['TUMOR', 'HEALTHY'] if nb_classes ==  2 else ['TUMOR','STROMA','COMPLEX','LYMPHO','DEBRIS','MUCOSA','ADIPOSE','EMPTY']

print(metrics.classification_report(y_test, y_pred, target_names=target_names))